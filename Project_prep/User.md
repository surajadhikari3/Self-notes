Absolutely. Here's a **documentation-ready explanation** of **group-level `GRANT`** and **user-level `REVOKE`** in **Databricks Unity Catalog**, including **who can perform the actions**, **how to do it**, and **important behavior notes** under the **additive access control model**.

---

# üîê Unity Catalog Permission Model: Group-Level Grant vs User-Level Revoke

## üìò Overview

In Unity Catalog, permissions are **additive**, meaning users gain access based on the **union of all grants** across user-level and group-level permissions. There is **no concept of DENY**, so **revoking a permission from a user does not block inherited access from a group**.

---

## üë§ Who Can Grant or Revoke Permissions?

|Role|Capabilities|
|---|---|
|**Metastore Admin**|Can manage all grants at any level|
|**Catalog Owner**|Can grant/revoke access within their catalog|
|**Schema Owner**|Can grant/revoke permissions on schemas and their child objects|
|**Table Owner**|Can grant/revoke access on specific tables or views|

> ‚ÑπÔ∏è Only users with **ownership** or **appropriate privileges** on a Unity Catalog object can issue `GRANT` or `REVOKE` statements.

---

## ‚úÖ How to Grant Permissions to a Group

### üìå SQL Syntax

```sql
GRANT SELECT ON TABLE catalog.schema.table_name TO `group_name`;
```

### üìç Example

```sql
GRANT SELECT ON TABLE sales_db.revenue.transactions TO `analysts`;
```

This grants `SELECT` privileges to all users in the group `analysts`.

---

## ‚ùå How to Revoke Permissions from a User

### üìå SQL Syntax

```sql
REVOKE SELECT ON TABLE catalog.schema.table_name FROM `user_email`;
```

### üìç Example

```sql
REVOKE SELECT ON TABLE sales_db.revenue.transactions FROM `john.doe@example.com`;
```

> üîî **Important**: If `john.doe@example.com` is a member of `analysts`, he will still retain `SELECT` access **via the group**.

---

## ‚ö†Ô∏è Key Behavior Notes under Additive Access Model

|Rule|Description|
|---|---|
|**Additive Permissions**|Permissions from multiple sources (user + group) are combined|
|**REVOKE is not DENY**|`REVOKE` removes a specific grant but does **not override** inherited access from a group|
|**No Negative Permissions**|You cannot explicitly deny access in Unity Catalog|
|**Group Grants > User Revokes**|Group-level grants will still apply even if the same permission is revoked at the user level|

---

## üö´ How to Restrict a User Who Inherits Access via a Group

If you need to **prevent a specific user from accessing** an object, even though their group has access:

|Option|Recommendation|
|---|---|
|‚úÖ **Remove user from the group**|Remove the user from the group that has access|
|‚úÖ **Create a custom group**|Create a group like `analysts_except_john` and assign only desired users|
|‚úÖ **Use row-level security**|Apply dynamic views or data masking to restrict access at query time|
|‚ö†Ô∏è Don‚Äôt rely on `REVOKE`|It does not block inherited access via groups|

---

## üîç Auditing Effective Permissions

To see what access a user has:

```sql
SHOW GRANTS ON TABLE catalog.schema.table_name;
```

To check **who has access and from which path** (user or group), use:

```sql
SHOW GRANTS TO `user_email` OR `group_name`;
```

---

## üß† Summary

|Task|SQL Command|Notes|
|---|---|---|
|Grant permission to group|`GRANT SELECT ON TABLE x TO group_name`|Grants access to all members|
|Revoke from individual|`REVOKE SELECT ON TABLE x FROM user_email`|Does not remove inherited access|
|Restrict a user fully|Remove from group / Apply data policy|Additive model doesn‚Äôt allow deny|

---

Let me know if you'd like this formatted into a **Markdown**, **Confluence**, or **PDF-friendly** format with diagrams.



For the foreign Catalog have to create the connection first and then select the type as a foreign catalog
![](../Pasted%20image%2020250708171659.png)


![](../Pasted%20image%2020250708171734.png)







To read data from your **SQL Server** into **Databricks using JDBC batching**, follow the setup below. I‚Äôll tailor it specifically to your connection string and the `feed_gen` database you‚Äôve shown in the screenshots.

---

### ‚úÖ Step-by-Step: Read from SQL Server into Databricks Using JDBC with Batching

---

### üîó 1. **JDBC URL Breakdown**

From your text file:

```text
jdbc:sqlserver://siechosdvyri0.d2-tdbfg.com:3341;encrypt=true;trustServerCertificate=true;databaseName=feed_gen;domain=D2-TDBFG;user=TECHO702SQLC;password=dsfdf;integratedSecurity=true;authenticationScheme=NTLM;
```

You should **adjust** this into a JDBC URL usable from Spark:

```python
jdbc_url = (
    "jdbc:sqlserver://siechosdvyri0.d2-tdbfg.com:3341;"
    "databaseName=feed_gen;"
    "encrypt=true;"
    "trustServerCertificate=true;"
    "authenticationScheme=NTLM"
)
```

---

### üì¶ 2. **Dependencies**

Make sure you have the **Microsoft JDBC driver** available in your Databricks cluster:

- Go to **Cluster > Libraries > Install New > Maven** and add:
    
    ```
    com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre8
    ```
    

---

### üìÑ 3. **Read with JDBC and Batching in PySpark**

Here‚Äôs a full **Databricks notebook** code snippet to read from a SQL Server table with batching:

```python
jdbc_url = (
    "jdbc:sqlserver://siechosdvyri0.d2-tdbfg.com:3341;"
    "databaseName=feed_gen;"
    "encrypt=true;"
    "trustServerCertificate=true;"
    "authenticationScheme=NTLM"
)

table_name = "db_datawriter.your_table_name"  # Replace with actual table name

connection_properties = {
    "user": "TECHO702SQLC",
    "password": "dsfdf",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver",
    "domain": "D2-TDBFG",
    # Batching options
    "fetchsize": "1000"  # Default is 100, increase for batch reads
}

df = spark.read.jdbc(
    url=jdbc_url,
    table=table_name,
    properties=connection_properties
)

df.show()
```

---

### ‚öôÔ∏è 4. **Optional: Partitioned Read (for Parallelism)**

To scale reading for large tables, you can add partitioning:

```python
df = spark.read.jdbc(
    url=jdbc_url,
    table=table_name,
    column="id",  # Replace with a numeric column like primary key
    lowerBound=1,
    upperBound=100000,
    numPartitions=8,
    properties=connection_properties
)
```

---

### üì• 5. **Save as Delta (Optional)**

```python
df.write.format("delta").mode("overwrite").save("/mnt/bronze/feed_gen/your_table_name")
```

---

### ‚úÖ Final Checklist

|Requirement|Status|
|---|---|
|JDBC Driver Installed|‚úÖ Yes|
|Correct JDBC URL Format|‚úÖ Cleaned|
|Authentication (NTLM/Domain)|‚úÖ Provided|
|Table Name Identified|‚õîÔ∏è Replace `your_table_name`|
|Partition Column (for scaling)|‚õîÔ∏è Choose numeric ID|

---

Would you like me to help you find the table name and ideal partition column from your `feed_gen` DB schema?