
| Option                                         | When to Use                                                                             | What It Does                                                     | Best For                                                    |
| ---------------------------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ----------------------------------------------------------- |
| **ðŸ”¹ Create Job** (from a notebook)            | You're using classic **PySpark/SQL notebooks** with `.readStream`, `.writeStream`, etc. | Runs your code as-is using a defined cluster                     | âœ… Manual or scheduled execution of **custom notebook code** |
| **ðŸ”¹ Create ETL Pipeline** (Delta Live Tables) | You're using **DLT syntax** like `@dlt.table` or `CREATE LIVE TABLE ...`                | Runs a **DLT-managed pipeline**, tracks table lineage & metadata | âœ… Declarative & managed **DLT workflows**                   |


## âœ… Part 1: Ready-to-Use PySpark ETL Notebook (Bronze â†’ Silver â†’ Gold)

### ðŸ§¾ File Structure Assumption:

- Data lands in: `abfss://raw@<storage>.dfs.core.windows.net/orders/`
    
- Output to Silver: `abfss://silver@<storage>.dfs.core.windows.net/orders_clean/`
    
- Output to Gold: `abfss://gold@<storage>.dfs.core.windows.net/orders_summary/`
    
- Checkpoint folders are placed in a dedicated `checkpoints` container
    

---

### ðŸ’¡ ETL Notebook Template (PySpark)

```python
from pyspark.sql.functions import current_timestamp, window, expr

# ========== BRONZE: Raw Ingestion ==========
df_bronze = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "parquet") \
    .option("cloudFiles.schemaLocation", "abfss://checkpoints@<storage>.dfs.core.windows.net/schema/orders/") \
    .load("abfss://raw@<storage>.dfs.core.windows.net/orders/")

# ========== SILVER: Data Cleaning ==========
df_silver = df_bronze \
    .filter("order_status IS NOT NULL") \
    .withColumn("ingested_at", current_timestamp())

silver_writer = df_silver.writeStream \
    .format("delta") \
    .option("checkpointLocation", "abfss://checkpoints@<storage>.dfs.core.windows.net/silver/orders/") \
    .outputMode("append") \
    .start("abfss://silver@<storage>.dfs.core.windows.net/orders_clean/")

# ========== GOLD: Aggregated Metrics ==========
df_gold = df_silver \
    .withWatermark("ingested_at", "10 minutes") \
    .groupBy(window("ingested_at", "1 hour"), "region") \
    .agg(expr("percentile_approx(order_amount, 0.5) as median_order"))

gold_writer = df_gold.writeStream \
    .format("delta") \
    .option("checkpointLocation", "abfss://checkpoints@<storage>.dfs.core.windows.net/gold/orders/") \
    .outputMode("complete") \
    .start("abfss://gold@<storage>.dfs.core.windows.net/orders_summary/")
```

> Replace `<storage>` with your actual storage account name.

---

## âœ… Part 2: How to Use Delta Live Tables (DLT)

---

### ðŸ§  What is DLT?

**Delta Live Tables (DLT)** is a Databricks feature that lets you define ETL pipelines **declaratively**, manage dependencies, track quality, and handle orchestration automatically.

It uses:

- `CREATE LIVE TABLE ...` to define views/tables
    
- SQL or Python decorators like `@dlt.table` in notebooks
    

---

### ðŸ§ª Example: SQL-based DLT Pipeline

```sql
-- Bronze: Raw orders
CREATE LIVE TABLE bronze_orders
AS SELECT * FROM cloud_files(
  "abfss://raw@<storage>.dfs.core.windows.net/orders/",
  "parquet"
);

-- Silver: Cleaned orders
CREATE LIVE TABLE silver_orders
AS SELECT * FROM LIVE.bronze_orders
WHERE order_status IS NOT NULL;

-- Gold: Aggregated orders
CREATE LIVE TABLE gold_orders
AS SELECT
  region,
  window(ingested_at, "1 hour") AS hour_window,
  percentile_approx(order_amount, 0.5) AS median_order
FROM LIVE.silver_orders
GROUP BY region, window(ingested_at, "1 hour");
```

---

### ðŸ§° How to Create a DLT Pipeline

1. **Create Notebook** with SQL or Python + DLT logic
    
2. Go to **Workflows > Delta Live Tables**
    
3. Click **Create Pipeline**
    
4. Choose:
    
    - Source notebook
        
    - Target schema/database
        
    - Pipeline storage location (for checkpoints & metadata)
        
5. Click **Start** to run
    

DLT will:

- Handle dependencies
    
- Validate schema
    
- Auto-create or update tables
    
- Restart automatically if there's failure
    

---

### âœ… Benefits of DLT

| Feature               | Benefit                                                   |
| --------------------- | --------------------------------------------------------- |
| Declarative syntax    | No need to manage `readStream` and `writeStream` manually |
| Managed checkpoints   | Automatic recovery                                        |
| Built-in data quality | With `EXPECT` statements                                  |
| DAG view              | Visual flow of Bronze â†’ Silver â†’ Gold                     |
| Versioning            | Integrated with Delta Lake for rollback and audit         |

---


Excellent question â€” this is a **core concept** when working with **Azure Databricks** or any **modern lakehouse architecture**.

---

## ðŸ§  Unity Catalog vs Hive Metastore â€” Key Differences

|Feature|**Unity Catalog** (UC)|**Hive Metastore** (HMS)|
|---|---|---|
|ðŸ” **Access control**|Centralized, fine-grained, role-based (users, groups, service principals)|Table/column-level access via legacy ACLs or workspace-scoped permissions|
|ðŸ—‚ **Multi-workspace support**|âœ… Yes (shared across all workspaces in a region)|âŒ No (each workspace has its own HMS)|
|ðŸ—ƒ **Data governance**|Built-in lineage, auditing, and data classification|âŒ Manual and scattered|
|ðŸ“ **Object hierarchy**|Catalog > Schema > Table|Database > Table (no catalogs)|
|ðŸ§¾ **Table types**|Managed + External + Delta Sharing native|Mostly external/managed, no native sharing|
|ðŸ‘¥ **Identity integration**|Uses **Azure Active Directory** (AAD) identities and SCIM groups|Workspace-local users/groups only|
|ðŸ”„ **Support for Delta Sharing**|âœ… Yes|âŒ No|
|ðŸ”Ž **Data lineage UI**|âœ… Yes (preview)|âŒ No|
|ðŸ“ **Location awareness**|Region-bound metastore with explicit locations for external storage|Loosely coupled to DBFS paths|
|ðŸ§± **Security enforcement**|Column/row-level security, masking via grants|Only basic grants at object level|

---

## ðŸ§© Architecture Diagram (Simplified)

### Hive Metastore (Legacy):

```
Workspace
 â””â”€â”€ Hive Metastore
      â”œâ”€â”€ Database
      â”‚    â””â”€â”€ Table (no centralized control)
```

### Unity Catalog:

```
Metastore (region-wide)
 â””â”€â”€ Catalog
      â””â”€â”€ Schema
           â””â”€â”€ Table / View / Function
```

- **Workspace-agnostic**
    
- **Region-specific**
    
- Supports **centralized identity + permission model**
    

---

## ðŸ” Access Control Comparison

|Level|Hive Metastore|Unity Catalog|
|---|---|---|
|Table-level|âœ… (via GRANT)|âœ…|
|Column-level|âŒ|âœ…|
|Row-level|âŒ|âœ… (via row filters)|
|Schema-level|âœ…|âœ…|
|Cross-workspace sharing|âŒ|âœ…|

---

## ðŸ’¬ When to Use What?

|Use Case|Recommendation|
|---|---|
|Legacy workloads (before UC)|Hive Metastore|
|Multi-workspace governance|âœ… Unity Catalog|
|Fine-grained access control|âœ… Unity Catalog|
|Delta Sharing across teams|âœ… Unity Catalog|
|Lakehouse security + audit|âœ… Unity Catalog|

---

## ðŸŽ¯ Final Verdict

> Unity Catalog is the **modern, unified metadata + governance layer**, designed to **replace the Hive Metastore** in all modern Databricks environments.  
> It brings **enterprise-grade security, lineage, and centralized control** across all workspaces and teams.

---

Would you like a cheat sheet showing CLI/SQL examples of granting access in Unity Catalog vs Hive Metastore?