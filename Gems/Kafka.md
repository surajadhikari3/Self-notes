  
Kafka streams DSL â€”> Stateless and stateful transformation can be done here**
Â **vs Processor Api -> Define the topology and add the source node, other nodes and marked the termination with sink node. â€”> it lets to create the own DAG(Direct acyclic graph ) for custom use case while designing the stream processing application..**Â 

  

Consumer producer app  
  
Native kafka client library jar for producing consuming (Just using the java not the spring boot like framework)  
  
Avro â€”>

Â Binary format ( 1 0 )

Faster serilization and desirilizarion since it is binary formatÂ 

Save 10 x storage and computingâ€¦.

Mssg validation against transaction (it compares against the schema registry and validates)

  
100 tps â€”> AboutÂ  100 million records process in kafka  

1 million message should process under 5 minÂ 

**Kafka. Streams**

Declarative vs imperative

Topology  
  
Operator in kaka streams  
  
mapValues(values ->  
map(key, value) ->  
filter((key, value) ->Â  Long.parseValue(value) > 1000)  
  
Kstream -> can subscripe to the multiple topicâ€¦  
KTable â€”> it can only subscribe to the single table  
  
  
serdes -> Serializer/Deserializerâ€¦..(Combined form of the serializer and deserializer)  
  
Joinsâ€¦..

### ğŸ” **KStream vs KTable: Key Differences**

| Feature       | `KStream`               | `KTable`                            |
| ------------- | ----------------------- | ----------------------------------- |
| Nature        | Stream of records       | Table with the latest value per key |
| Records       | Append-only (event log) | Keyed updates (changelog stream)    |
| Use Cases     | Event processing        | Aggregation, lookups, joins         |
| Example       | Orders, logs            | Inventory counts, user state        |
| Joins With    | `KStream`, `KTable`     | `KTable`, `KStream`                 |
| Re-processing | All events              | Only latest key-value               |


### ğŸ§  Real-world Analogy:

| Concept   | Analogy                                                                           |
| --------- | --------------------------------------------------------------------------------- |
| `KStream` | Like a **transaction log** â€” every line is an action (e.g., "User bought item X") |
| `KTable`  | Like a **database table** â€” for each user, only the **latest balance** is kept    |


Diagram: Visualizing KStream & KTable Joins

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        KStream             â”‚  âœ Order Events
               â”‚  Topic: orders             â”‚  e.g., (user1, "order#123")
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 (Stream of Order Events)

     JOIN with ğŸ‘‡

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚         KTable             â”‚  âœ User Profiles (latest only)
               â”‚  Topic: user_profiles      â”‚  e.g., (user1, "Gold Member")
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 (Table of latest user state)

                            â”‚
                            â–¼

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Enriched KStream Output  â”‚
               â”‚  Topic: enriched_orders    â”‚
               â”‚  (user1, order#123 + Gold) â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



### **A. KStream â†’ KTable Join**

**Use Case**: Enriching real-time events with user or product metadata.

|Example|Why Itâ€™s Useful|
|---|---|
|Join orders (KStream) with user profile (KTable)|Add loyalty status, shipping preferences before fulfillment.|
|Join purchase stream with product info table|Attach price, category, inventory status.|

ğŸ“ **Code Snippet**:


```
KStream<String, Order> orderStream = builder.stream("orders");
KTable<String, UserProfile> profileTable = builder.table("user_profiles");

KStream<String, EnrichedOrder> enriched = orderStream.join(
    profileTable,
    (order, profile) -> new EnrichedOrder(order, profile)
);

```

---

### âœ… **B. KStream â†’ KStream Join**

**Use Case**: Correlating two event streams within a **time window**.

|Example|Why Itâ€™s Useful|
|---|---|
|Join payment events with order events|Detect if payment came within 10 minutes of order.|
|Join app login and location streams|Track user activity and geo-location together.|

ğŸ“ **Code Snippet**:

java

CopyEdit

```
KStream<String, Order> orders = builder.stream("orders");
KStream<String, Payment> payments = builder.stream("payments");

KStream<String, Matched> matched = orders.join(
    payments,
    (order, payment) -> new Matched(order, payment),
    JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofMinutes(5))
);

```

---

### âœ… **C. KTable â†’ KTable Join**

**Use Case**: Building a unified view of system state.

|Example|Why Itâ€™s Useful|
|---|---|
|Join user settings with preferences|Build a complete settings profile.|
|Join inventory count with pricing|Dashboard for retail managers.|

ğŸ“ **Code Snippet**:


```
KTable<String, Inventory> inventoryTable = builder.table("inventory");
KTable<String, Pricing> pricingTable = builder.table("pricing");

KTable<String, ProductStatus> productStatus = inventoryTable.join(
    pricingTable,
    (inventory, pricing) -> new ProductStatus(inventory, pricing)
);

```

---

## ğŸ“Œ Summary Table: Join Type Use Cases

|Join Type|Left Side|Right Side|Real-World Example|
|---|---|---|---|
|`KStream-KTable`|Stream|Lookup|Enrich order with user profile|
|`KStream-KStream`|Stream|Stream|Match order and payment within time window|
|`KTable-KTable`|Table|Table|Merge pricing and inventory data|

## ğŸªŸ **Types of Windows in Kafka Streams**

Kafka provides several types of windows depending on how you want to group the events in time:

| Window Type          | Description                                                             | Real-world Example                                  |
| -------------------- | ----------------------------------------------------------------------- | --------------------------------------------------- |
| **Tumbling Window**  | Fixed-size, **non-overlapping** windows                                 | Count page views **per minute**                     |
| **Hopping Window**   | Fixed-size, **overlapping** windows with hop (slide) interval           | Count logins in a **5-min window every 1 min**      |
| **Sliding Window**   | Windows **based on difference between event timestamps** (dynamic size) | Match related events like **click â†’ purchase**      |
| **Session Window**   | **Inactivity-gap-based** dynamic window                                 | User activity session â€” ends if idle for 30 seconds |
| **Unlimited Window** | Infinite window (default) for non-windowed aggregations                 | Count all-time messages (until app is stopped)      |
## ğŸ”¥ **Most Commonly Used Windows**

| Use Case                          | Common Window Type  |
| --------------------------------- | ------------------- |
| Real-time dashboard (rolling avg) | **Hopping Window**  |
| Event correlation                 | **Sliding Window**  |
| Periodic reporting                | **Tumbling Window** |
| User session behavior             | **Session Window**  |
| Lifetime counters                 | **Unlimited**       |
## Visual Overview

### Tumbling vs Hopping vs Sliding

Time Axis â¡

Tumbling:  [-----] [-----] [-----]
Hopping:   [-----]
             [-----]
               [-----]
Sliding:    [---]   [---]   [---]   (based on time difference between two records)


### Acknowledgment + Processing Guarantees

| Guarantee         | Producer Setting                        | Consumer Offset Strategy           |
| ----------------- | --------------------------------------- | ---------------------------------- |
| **At-most-once**  | `acks=0` or `1`                         | Auto-commit (may skip processing)  |
| **At-least-once** | `acks=all`                              | Manual commit **after** processing |
| **Exactly-once**  | `acks=all` + Idempotence + Transactions | Transactions + commit within scope |
## ğŸ“Œ Summary Table

| Role     | Type          | What It Does                             | Config / API                           |
| -------- | ------------- | ---------------------------------------- | -------------------------------------- |
| Producer | Acks          | Controls write durability to Kafka       | `acks=0/1/all`                         |
| Consumer | Offset Commit | Controls read/processing confirmation    | `enable.auto.commit`, `commitSync()`   |
| Streams  | EOS           | Guarantees once-and-only-once processing | `processing.guarantee=exactly_once_v2` |

## **Consumer Acknowledgements (Offset Commits)**

Kafka doesnâ€™t **auto-track whatâ€™s been read** â€” **consumers commit offsets** to mark progress.

### âœ… Types of Offset Commits:

| Type                    | Description                                                         |
| ----------------------- | ------------------------------------------------------------------- |
| **Automatic commit**    | Offsets are committed at intervals (e.g., every 5 sec)              |
| **Manual commit**       | Developer explicitly commits offset after successful processing     |
| **Synchronous commit**  | `commitSync()` â€” waits for broker ack, retries on failure           |
| **Asynchronous commit** | `commitAsync()` â€” doesnâ€™t wait; faster but **may risk offset loss** |